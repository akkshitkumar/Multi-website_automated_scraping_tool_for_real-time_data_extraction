## Multi-website automated scraping tool for real-time data extraction.

Problem/ Opportunity
The managerial problem/ opportunity being addressed
A data analytics team at EY GVT Gurgaon is needed to analyze information and conduct analytics and strategic research based on the data and requirements of the client project and internal forensics teams. They use SSIS, Python, SQL, Power BI, Excel, and other BI tools to support and address these needs regularly. In accordance with the project timeline, they offer real-time, automated, generalized solutions to the complex problems at hand.
One such task/problem is extracting data for the clients or forensics teams (which works on generating leads, acquiring new projects/business from companies, and providing them with consultation and solutions on a regular basis). They need the information quickly produced by a single search and a few clicks to locate and validate these leads. This is an opportunity to learn and deliver a solution for the need for Web-scraping, which necessitates creating a tool for the end user's ease of use.

Project Objectives
•	The project’s primary objective is to scrape data from online sources.
•	Creating generalised tools with the help of Python libraries such as Scrappy, Selenium, Beautiful Soup, Pandas, NumPy, etc.
•	Combining scraping module and merging it with GUI to create an automated web-scraping tool/bot.
•	In this project, analysis and representations of relationships in the scraped data will also be attempted.
•	To perform Visualizations on data collected from various sources.
•	Lastly, learning and attempting to extract data using various sources and concepts from HTML, JSON, etc.

Methodology
The project will be carried out in phases. 
The methodology to be used, under the direction of the project and industry mentors, consists of logic building, script writing by breaking the tasks into small sections and joining them into the final script, debugging, data conversion and manipulation, along with some client project integration for better learning and understanding. The project is to be carried out in three phases:
Phase 1: For the tasks assigned to me as a Summer Intern, the most suitable tool shall be used as per the guidelines and requirements (mostly Python and Excel). The initial phase will start with understanding and practising the concepts to be used, project requirements and building the web-scraping scripts for the websites.
Phase 2: Generalizing the codes, manipulating data as per the requirements and building the initial GUI and tool for the end-user as writing code or a script for a web scraping automation program involves several challenges, including creating new functions, learning new modules, integrating new technology, preprocessing data (especially conversion), waiting for a website to load before extracting content or clicking on it, accurately extracting and mapping desired content, extracting tables, and more.
Phase 3: The web scraping, extraction, data manipulation and automation scripts will further be merged to deliver the final GUI program as a tool for easy data extraction and also visualizing on it for analysis and decision making.

Plan and Tasks
Identified tasks.
•	Learning web scraping techniques on Python using the various libraries and modules used by the team or available online.
•	Creation of scraping modules for the required websites and content.
o	extraction and mapping of table data.
o	creating new data frames using concatenating, joining/ merging processes.
•	Debug the code if it becomes stuck or generates errors.
•	UI and tool development for the end-user.
•	Updating scripts as per new requirements and changes.
•	Work on complex python deep learning module on fraudulent detection for solving real-time industrial problems.
•	Two flow diagrammatic representations for relationships between organisations from the scraped data.
•	Performing data pre-processing tasks such as converting highly noisy complex pdfs to the required format, followed by further processing and consolidation with VBA and macros.


The plan for completion of the project.
WEEK	WORK
Week 1	- Learn and practice the concepts of web-scraping and extraction.
- Understand the project and requirements.
Week 2 – 6	- The initial phase of the data extraction.
- Build scraping modules for websites.
Week 3	- Model development.
Week 4 – 8	- Compilation of scraping scripts.
- Understand and practice the GUI concepts and libraries.
- Build user interface.
Week 5 – 8	- GUI automation for the tool.
- Visualization of the data scraped.
Week 8 – 9	- Run time testing for the tool and integration with scraping scripts.
Week 9	- Finalizing the tool and approval.

Outcomes
Impact of the project on the organization and the benefits for the trainee.
•	The project will assist in reducing the effort and time for end-users to search and extract the desired data.
•	It will also be used to assist the key decision-makers in the project/forensics in analysing the company/ business to target and plan their strategy.
•	With this internship, I expect to develop the skill to identify and provide solutions to complex industrial problems and gain knowledge of the requirements, interactions, and behaviour strategies for both client and internal projects.
•	Develop an understanding of the organisational or industrial workings of a data analytics team.
o	Understanding how data is generated or received, how it is pre-processed, and how it is used within an organisation.
•	Expand on my academic knowledge of technical, analytical, and managerial concepts and comprehend how they are applied in real-world workplace settings.
o	Learning about the languages and tools used by the company and the functionality of ones learned in the curriculum, such as Python, R, SQL, Excel, Tableau, and Power BI
o	as well as their advanced use.
•	Develop my teamwork, leadership, communication, analytical, time management, project management, and relationship-building skills.
